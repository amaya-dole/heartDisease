# -*- coding: utf-8 -*-
"""PreProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H2OXg2UyS6Zd7F0Z8b3ucA22nzvMbtMe

**Heart disease Prediction**
"""

# Commented out IPython magic to ensure Python compatibility.
# 01
# Importing libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import norm, skew
from scipy import stats
import statsmodels.api as sm

# sklearn modules for data preprocessing:
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#sklearn modules for Model Selection:
from sklearn import svm, tree, linear_model, neighbors
from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
#from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

#sklearn modules for Model Evaluation & Improvement:    
from sklearn.metrics import confusion_matrix, accuracy_score 
from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import KFold
from sklearn import feature_selection
from sklearn import model_selection
from sklearn import metrics
from sklearn.metrics import classification_report, precision_recall_curve
from sklearn.metrics import auc, roc_auc_score, roc_curve
from sklearn.metrics import make_scorer, recall_score, log_loss
from sklearn.metrics import average_precision_score

#Standard libraries for data visualization:
import seaborn as sn
from matplotlib import pyplot
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import matplotlib 
# %matplotlib inline
color = sn.color_palette()
import matplotlib.ticker as mtick
from IPython.display import display
pd.options.display.max_columns = None
from pandas.plotting import scatter_matrix
from sklearn.metrics import roc_curve

#Miscellaneous Utilitiy Libraries:   
import random
import os
import re
import sys
import timeit
import string
import time
from datetime import datetime
from time import time
from dateutil.parser import parse
import joblib

#02
#importing the dataset
#from google.colab import drive
#drive.mount('/content/drive')
dataset = pd.read_csv('HeartDisease.csv')

#getting 1st five raws of the dataset
dataset.head()

#identify the columns
dataset.columns

#describing the dataset
dataset.describe()

#grouping the dataset according to the data types
dataset.columns.to_series().groupby(dataset.dtypes).groups

#03
#Taking care of null values
dataset.isnull()

#getting the sum of null values in each attribute
dataset.isnull().sum()

#Taking care of duplicate values
#data_dup = dataset.duplicated().any()
#data_dup

#Drop duplicates
#data = dataset.drop_duplicates()

#data_dup = data.duplicated().any()
#data_dup

#getting no of raws and columns of the dataset
dataset.shape

#get co-relation of each feature
cor = dataset.corr()
top_cor_features = cor.index
plt.figure(figsize=(15,15))
#plotting the heat map
g = sn.heatmap(dataset[top_cor_features].corr(), annot = True, cmap = 'hot')

#Correlation with output variable
cor_target = abs(cor["target"])
#Selecting highly correlated features
relevant_features = []
relevant_features = cor_target[cor_target>0.2]
relevant_features

for feature in dataset.columns:
  if feature not in relevant_features:
    print(feature)

newDataset = dataset.drop(['trestbps','chol','fbs', 'restecg'], axis = 1)
newDataset

#04 
#Data Processing
cate_val=[]
cont_val=[]

for column in newDataset.columns:
  if newDataset[column].nunique() <= 10:
    cate_val.append(column)
  else:
    cont_val.append(column)

cate_val

cont_val

#Encoding categorical data
cate_val

newDataset['cp'].unique()

cate_val.remove('sex')
cate_val.remove('target')
dataset = pd.get_dummies(dataset,columns=cate_val,drop_first = True)

#05 
#Feature scaling
newDataset.head()

st = StandardScaler()
newDataset[cont_val]=st.fit_transform(newDataset[cont_val])

newDataset.head()

#8. Splitting the dataset into the training set and test set
x = newDataset.drop('target', axis=1)

y=newDataset['target']

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2,random_state=42)

x_train

x_test

y_train

y_test

"""**01. Logistic Regression**"""

#Logistic regression
newDataset.head()

log = LogisticRegression()
log.fit(x_train,y_train)

y_pred1 = log.predict(x_test)

accuracy_score(y_test,y_pred1)

"""**02. SVC**"""

svm = svm.SVC()

svm.fit(x_train, y_train)

y_pred2 = svm.predict(x_test)

accuracy_score(y_test, y_pred2)

"""**03. KNeighbors Classifier**"""

knn = KNeighborsClassifier()

knn.fit(x_train, y_train)

y_pred3 = knn.predict(x_test)

accuracy_score(y_test, y_pred3)

score = []

for k in range(1,40):
  knn = KNeighborsClassifier(n_neighbors=k)
  knn.fit(x_train, y_train)
  y_pred = knn.predict(x_test)
  score.append(accuracy_score(y_test, y_pred))

score

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
accuracy_score(y_test, y_pred)

"""**Non-Linear ML Algorithm**"""

#Non-Linear ML Algorithm
#dataset = pd.read_csv(r'/content/drive/My Drive/FDM Mini Project/heartD.csv')
#dataset.head()

#dataset = dataset.drop_duplicates()

#dataset.shape

#x = dataset.drop('target', axis=1)
#y = dataset['target']

#x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2,random_state=42)

accuracy_score(y_test,y_pred1)

"""**04. Decision Tree Classifier**"""

dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)

dt.fit(x_train, y_train)

y_pred4 = dt.predict(x_test)

accuracy_score(y_test, y_pred4)

"""**05. Random Forest Classifier**"""

rf = RandomForestClassifier(n_estimators = 10)

rf.fit(x_train, y_train)

y_pred5 = rf.predict(x_test)

accuracy_score(y_test, y_pred5)

final_data = pd.DataFrame({'Models' : ['LR', 'SVM', 'KNN', 'DT', 'RF'],
                           'ACC':[accuracy_score(y_test, y_pred1),
                                  accuracy_score(y_test, y_pred2),
                                  accuracy_score(y_test, y_pred3),
                                  accuracy_score(y_test, y_pred4),
                                  accuracy_score(y_test, y_pred5),]
                           })

final_data

import seaborn as sns
sns.barplot(final_data['Models'],final_data['ACC'])